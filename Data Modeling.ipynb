{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef71a5a1-2bbc-4ab9-9908-adbcf5e112ad",
   "metadata": {},
   "source": [
    "## Data Modeling\n",
    "Describe and justify the process for analyzing or modeling the data.\n",
    "\n",
    "Questions to consider:\n",
    "\n",
    "How did you analyze or model the data?\n",
    "How did you iterate on your initial approach to make it better?\n",
    "Why are these choices appropriate given the data and the business problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a071587-43a2-4ec0-890f-9d7725e63f21",
   "metadata": {},
   "source": [
    "# Contents\n",
    "1. Load the dataset\n",
    "2. Baseline model\n",
    "3. Model Iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bda2016-db70-429f-ae58-a237af87b073",
   "metadata": {},
   "source": [
    "## 1.Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55c2ecb0-aa0d-4b68-ab6e-e27cb0f51d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images: 5271\n",
      "Average width: 1330\n",
      "Average height: 972\n",
      "Average aspect ratio: 1.37\n",
      "Max width: 2890\n",
      "Max height: 2713\n",
      "Min width: 384\n",
      "Min height: 127\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def get_image_sizes(directory):\n",
    "    \"\"\"Collect widths and heights of images with given extensions in directory.\"\"\"\n",
    "    sizes = [(img.size) for root, dirs, files in os.walk(directory)\n",
    "             for file in files if file.lower().endswith('jpeg')\n",
    "             for img in (Image.open(os.path.join(root, file)),)]\n",
    "    return zip(*sizes)  # Unzips the sizes into two lists: widths and heights\n",
    "\n",
    "def print_image_statistics(widths, heights):\n",
    "    \"\"\"Print statistics for a collection of image widths and heights.\"\"\"\n",
    "    num_images = len(widths)\n",
    "    avg_width = round(np.mean(widths))\n",
    "    avg_height = round(np.mean(heights))\n",
    "    avg_aspect_ratio = np.mean(widths) / np.mean(heights)\n",
    "    max_width = max(widths)\n",
    "    max_height = max(heights)\n",
    "    min_width = min(widths)\n",
    "    min_height = min(heights)\n",
    "    \n",
    "    print(f\"Number of images: {num_images}\")\n",
    "    print(f\"Average width: {avg_width}\")\n",
    "    print(f\"Average height: {avg_height}\")\n",
    "    print(f\"Average aspect ratio: {avg_aspect_ratio:.2f}\")\n",
    "    print(f\"Max width: {max_width}\")\n",
    "    print(f\"Max height: {max_height}\")\n",
    "    print(f\"Min width: {min_width}\")\n",
    "    print(f\"Min height: {min_height}\")\n",
    "    return avg_aspect_ratio\n",
    "\n",
    "train_dir = 'data/chest_xray/new_train/'\n",
    "test_dir = 'data/chest_xray/new_test/'\n",
    "\n",
    "train_widths, train_heights = get_image_sizes(train_dir)\n",
    "test_widths, test_heights = get_image_sizes(test_dir)\n",
    "\n",
    "widths = np.concatenate((train_widths, test_widths))\n",
    "heights = np.concatenate((train_heights, test_heights))\n",
    "\n",
    "avg_ratio = print_image_statistics(widths, heights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a64a677-5f0d-415f-b3a9-4dfe8a6fc3f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-29 11:38:06.372205: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "tf.random.set_seed(42)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def create_dataset(directory, width, batch_size, ratio = avg_ratio):\n",
    "    height = int(width/ratio)\n",
    "    dataset = image_dataset_from_directory(directory,\n",
    "                                        label_mode='binary',\n",
    "                                        color_mode=\"grayscale\", # will save memory as images are already in grayscale\n",
    "                                        batch_size=batch_size,\n",
    "                                        image_size=(height,width),\n",
    "                                        shuffle=True,\n",
    "                                        crop_to_aspect_ratio = True,\n",
    "                                        seed=42)\n",
    "    return dataset\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Rescaling\n",
    "\n",
    "def process_dataset(dataset):\n",
    "    # Define the rescaling layer\n",
    "    rescale = Rescaling(1./255)\n",
    "    \n",
    "    # Normalizing the dataset\n",
    "    dataset = dataset.map(lambda x, y: (rescale(x), y))\n",
    "    \n",
    "    # improves speed by only having to read the dataset for the first epoch\n",
    "    dataset = dataset.cache()\n",
    "    \n",
    "    # increases generalization by shuffling elements each epoch\n",
    "    dataset = dataset.shuffle(buffer_size=1000, seed=42)\n",
    "    \n",
    "    # this automatically adjusts the number of batches\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6e1f221-6e6b-4af2-aad0-1f00a9fc2eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4684 files belonging to 2 classes.\n",
      "Found 585 files belonging to 2 classes.\n",
      "Found 587 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_dir = \"data/chest_xray/new_train\"\n",
    "val_dir = \"data/chest_xray/new_val\"\n",
    "test_dir = \"data/chest_xray/new_test\"\n",
    "\n",
    "\n",
    "train_ds = create_dataset(train_dir, width=256, batch_size=32,ratio = avg_ratio)\n",
    "val_ds = create_dataset(val_dir, width=256, batch_size=32,ratio = avg_ratio)\n",
    "test_ds = create_dataset(test_dir, width=256, batch_size=32,ratio = avg_ratio)\n",
    "\n",
    "train_ds_scaled = process_dataset(train_ds)\n",
    "val_ds_scaled = process_dataset(val_ds)\n",
    "test_ds_scaled = process_dataset(test_ds)\n",
    "\n",
    "input_shape = (187, 256, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "916c78df-5665-409e-8335-c6579daac32d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_PrefetchDataset element_spec=(TensorSpec(shape=(None, 187, 256, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None, 1), dtype=tf.float32, name=None))>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1732b14c-35fb-48bc-8fc6-96e52e5a2956",
   "metadata": {},
   "source": [
    "For metrics, I chose accuracy to show overall performance of the model, Recall to emphasise the importance of minimizing false negatives, and AUC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28b7e6a3-73ec-45b0-ba8a-05126516d323",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.metrics import BinaryAccuracy, Recall, AUC\n",
    "metrics=[BinaryAccuracy(name='accuracy'),\n",
    "         Recall(name='recall'),\n",
    "         AUC(name='auc')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b4aef1-c8df-4122-b72a-ee0bd167708a",
   "metadata": {},
   "source": [
    "## Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8eb5fa39-23c4-4a85-934f-2a1fe79af5f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 19.46078085899353 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.metrics import AUC\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Rescaling, RandomZoom\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.regularizers import l1, l2\n",
    "import time\n",
    "\n",
    "baseline_model = Sequential([\n",
    "    Flatten(input_shape=input_shape),\n",
    "    Dense(10, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "baseline_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=metrics)\n",
    "start_time = time.time()\n",
    "baseline_results = baseline_model.fit(train_ds_scaled,\n",
    "                                      epochs=10,\n",
    "                                      validation_data=val_ds_scaled,\n",
    "                                      verbose=0)\n",
    "end_time = time.time()\n",
    "duration = end_time - start_time\n",
    "print(f\"Training time: {duration} seconds\")\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925dcb20-7397-4269-af00-6c427bc08747",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6725af3b-ac4b-4afd-9774-8cacd21faeeb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "08d68370-a3cc-4572-832e-e2fe09715213",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "436f9ccc-34d4-46f7-aa0f-4d21e2c52965",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f91ec299-dd2f-42d6-add6-8fbf587bc84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.metrics import AUC\n",
    "from tensorflow.keras.utils import image_dataset_from_directory\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Rescaling, RandomZoom\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.regularizers import l1, l2\n",
    "from tensorflow.keras import layers\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "import time\n",
    "import random\n",
    "random.seed(42)\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "metrics=[tf.keras.metrics.AUC(curve = 'PR', name='auc_pr'),\n",
    "         tf.keras.metrics.AUC(name='auc_'),\n",
    "         'accuracy',\n",
    "         tf.keras.metrics.Precision(name='precision'),\n",
    "         tf.keras.metrics.Recall(name='recall')]\n",
    "\n",
    "def visualize_training_results(history,num_epochs):\n",
    "    metric = list(history.history.keys())[1]\n",
    "    train_score = history.history[metric]\n",
    "    val_score = history.history['val_'+metric]\n",
    "    \n",
    "    train_loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    \n",
    "    epochs_range = range(num_epochs)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs_range, train_score, label='Training '+metric)\n",
    "    plt.plot(epochs_range, val_score, label='Validation '+metric)\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.title('Training and Validation '+metric)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs_range, train_loss, label='Training Loss')\n",
    "    plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.show()\n",
    "    \n",
    "def baseline_modeler(model, metrics = 'accuracy', optimizer = 'adam', num_epochs=100, early_stopping = None ):\n",
    "    \n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=[metrics])\n",
    "    start_time = time.time()\n",
    "    if early_stopping == None:\n",
    "        history = model.fit(train_ds,\n",
    "                            epochs=num_epochs,\n",
    "                            validation_data=val_ds,\n",
    "                            verbose=0)\n",
    "    else:\n",
    "        history = model.fit(train_ds,\n",
    "                            epochs=num_epochs,\n",
    "                            validation_data=val_ds,\n",
    "                            verbose=0,\n",
    "                            callbacks = [early_stopping])\n",
    "        \n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    print(f\"Training time: {duration} seconds\")\n",
    "    print('')\n",
    "    visualize_training_results(history, num_epochs=int(len(history.history['loss'])))\n",
    "    print('')\n",
    "    train_scores = model.evaluate(train_ds)\n",
    "    val_scores = model.evaluate(val_ds)\n",
    "    num_metrics = int(len(history.history.keys())/2)\n",
    "    metrics_names = list(history.history.keys())[:num_metrics]\n",
    "    diff_scores = [b - a for a, b in zip(train_scores, val_scores)]\n",
    "    display(pd.DataFrame([train_scores,val_scores,diff_scores],index=['Train','Val','Diff'],columns=metrics_names))\n",
    "    print('------------------------------')\n",
    "    print('')\n",
    "\n",
    "def create_dataset(directory):\n",
    "    dataset = image_dataset_from_directory(directory,\n",
    "                                        label_mode='binary',\n",
    "                                        color_mode=\"grayscale\", # will save memory as images are already in grayscale\n",
    "                                        batch_size=32, # selecting default value\n",
    "                                        image_size=(256,256), # selecting default value\n",
    "                                        shuffle=True,\n",
    "                                        seed=42)\n",
    "    return dataset\n",
    "def process_dataset(dataset):\n",
    "    # Define the rescaling layer\n",
    "    rescale = Rescaling(1./255)\n",
    "    \n",
    "    # Normalizing the dataset\n",
    "    dataset = dataset.map(lambda x, y: (rescale(x), y))\n",
    "    \n",
    "    # improves speed by only having to read the dataset for the first epoch\n",
    "    dataset = dataset.cache()\n",
    "    \n",
    "    # increases generalization by shuffling elements each epoch\n",
    "    dataset = dataset.shuffle(buffer_size=1000, seed=42)\n",
    "    \n",
    "    # this automatically adjusts the number of batches\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "85a06e50-fd3e-4c74-badc-c2489c9cac80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4684 files belonging to 2 classes.\n",
      "Found 587 files belonging to 2 classes.\n",
      "Found 585 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Paths to the images\n",
    "train_dir = \"data/chest_xray/new_train\"\n",
    "test_dir = \"data/chest_xray/new_test\"\n",
    "val_dir = \"data/chest_xray/new_val\"\n",
    "\n",
    "train_ds_raw = create_dataset(train_dir)\n",
    "test_ds_raw = create_dataset(test_dir)\n",
    "val_ds_raw = create_dataset(val_dir)\n",
    "\n",
    "train_ds = process_dataset(train_ds_raw)\n",
    "test_ds = process_dataset(test_ds_raw)\n",
    "val_ds = process_dataset(val_ds_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2226ea-5526-4941-985f-d16ef481b8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Rescaling(1./255, input_shape= input_shape),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=metrics)\n",
    "results = model.fit(train_ds,\n",
    "                    epochs=10,\n",
    "                    validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "083692c1-10e3-47e3-972f-a4259d1cdc76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74/74 [==============================] - 4s 43ms/step - loss: 7.6212e-05 - auc_pr: 1.0000 - auc_: 1.0000 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000\n",
      "10/10 [==============================] - 0s 39ms/step - loss: 0.2287 - auc_pr: 0.9882 - auc_: 0.9800 - accuracy: 0.9607 - precision: 0.9676 - recall: 0.9789\n"
     ]
    }
   ],
   "source": [
    "# Baseline, no initializers\n",
    "model = Sequential([\n",
    "    Rescaling(1./255, input_shape= input_shape),\n",
    "    Conv2D(32, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=metrics)\n",
    "# model.summary()\n",
    "# results = model_1.fit(train_ds, \n",
    "#                         train_labels,\n",
    "#                         epochs=50,\n",
    "#                         validation_data=val_ds)\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Path to the saved model\n",
    "model_path = 'models/1_baseline.h5'\n",
    "\n",
    "# Load the model\n",
    "model = load_model(model_path)\n",
    "\n",
    "# After loading, you can use the model to make predictions, evaluate it, etc.\n",
    "train_scores = model.evaluate(train_ds)\n",
    "val_scores = model.evaluate(val_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b7dfbb6-61de-4bbb-a8d0-409d7b26531d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74/74 [==============================] - 4s 53ms/step - loss: 7.6212e-05 - auc_pr: 1.0000 - auc_: 1.0000 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000\n",
      "10/10 [==============================] - 1s 50ms/step - loss: 0.2287 - auc_pr: 0.9882 - auc_: 0.9800 - accuracy: 0.9607 - precision: 0.9676 - recall: 0.9789\n"
     ]
    }
   ],
   "source": [
    "train_scores = model.evaluate(train_ds)\n",
    "val_scores = model.evaluate(val_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01b5fa6e-a50a-45b4-871b-361b3cbf3379",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>auc_pr</th>\n",
       "      <th>auc</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precicion</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.22866</td>\n",
       "      <td>0.988204</td>\n",
       "      <td>0.980034</td>\n",
       "      <td>0.960684</td>\n",
       "      <td>0.967593</td>\n",
       "      <td>0.978923</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      loss    auc_pr       auc  accuracy  precicion    recall\n",
       "0  0.22866  0.988204  0.980034  0.960684   0.967593  0.978923"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "display(pd.DataFrame([val_scores],columns=['loss', 'auc_pr','auc','accuracy','precicion','recall']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3faf5573-92f5-4962-8544-de4713d16c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildModel(optimizer):\n",
    "    # Initialising the ANN\n",
    "    classifier = models.Sequential()\n",
    "    \n",
    "    # Adding the input layer and the first hidden layer\n",
    "    classifier.add(layers.Dense(128, activation = 'relu', input_shape=(49152,))) \n",
    "    \n",
    "    # Adding the second hidden layer\n",
    "    classifier.add(layers.Dense(32, activation='relu'))\n",
    "        \n",
    "    # Adding the output layer\n",
    "    classifier.add(layers.Dense(5, activation='sigmoid'))\n",
    "    \n",
    "    # Compiling the ANN\n",
    "    classifier.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['acc'])\n",
    "    \n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84fcbb2-f376-42a0-80bd-59e46bd6992b",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_classifier = KerasRegressor(build_fn = buildModel_binary)\n",
    "#What hyperparameter we want to play with\n",
    "parameters = {'batch_size': [16, 32, 64, 128],\n",
    "              'epochs': [5, 10, 50, 100],\n",
    "              'optimizer': ['adam', 'sgd' 'rmsprop']}\n",
    "binary_grid_search = GridSearchCV(estimator = binary_classifier,\n",
    "                           param_grid = parameters,\n",
    "                           scoring = 'neg_mean_absolute_error',\n",
    "                           cv = 5)\n",
    "binary_grid_search = binary_grid_search.fit(binary_train, binary_train_labels, verbose = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96569ff6-02a1-410a-9870-7ad7d1d705c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same model with he_normal initializer\n",
    "# Best initializer based on val_loss value, despite overfitting for all\n",
    "model = Sequential([\n",
    "    \n",
    "    Rescaling(1./255, input_shape= input_shape),\n",
    "    \n",
    "    Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    Flatten(),\n",
    "    \n",
    "    Dense(128, activation='relu', kernel_initializer='he_normal'),\n",
    "    \n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "baseline_modeler(model, metrics = metrics, optimizer = 'adam', num_epochs=50, early_stopping = None )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bfe924-b0c9-4183-8afe-beeec022e02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lecun_normal initializer\n",
    "\n",
    "model = Sequential([\n",
    "    \n",
    "    Rescaling(1./255, input_shape= input_shape),\n",
    "    \n",
    "    Conv2D(32, (3, 3), activation='relu', kernel_initializer='lecun_normal'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    Conv2D(64, (3, 3), activation='relu', kernel_initializer='lecun_normal'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    Conv2D(128, (3, 3), activation='relu', kernel_initializer='lecun_normal'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    Flatten(),\n",
    "    \n",
    "    Dense(128, activation='relu', kernel_initializer='lecun_normal'),\n",
    "    \n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "baseline_modeler(model, metrics = metrics, optimizer = 'adam', num_epochs=50, early_stopping = None )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85604f51-4d2b-4cd3-8d23-8462d056fed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD optimizer with he_normal initializer\n",
    "# Beat out adam by a good amount and killed rmsprop\n",
    "\n",
    "model = Sequential([\n",
    "    \n",
    "    Rescaling(1./255, input_shape= input_shape),\n",
    "    \n",
    "    Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    Flatten(),\n",
    "    \n",
    "    Dense(128, activation='relu', kernel_initializer='he_normal'),\n",
    "    \n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "baseline_modeler(model, metrics = metrics, optimizer = 'SGD', num_epochs=50, early_stopping = None )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66db3c61-4344-4e82-b006-d479d2be2709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSprop optimizer with he_normal initializer\n",
    "model = Sequential([\n",
    "    \n",
    "    Rescaling(1./255, input_shape= input_shape),\n",
    "    \n",
    "    Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    Flatten(),\n",
    "    \n",
    "    Dense(128, activation='relu', kernel_initializer='he_normal'),\n",
    "    \n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "baseline_modeler(model, metrics = metrics, optimizer = 'rmsprop', num_epochs=50, early_stopping = None )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31174ac-8681-4d6a-bd28-93a4579e5fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# l1 regularizer, SGD optimizer, he_normal initializer\n",
    "\n",
    "model = Sequential([\n",
    "    \n",
    "    Rescaling(1./255, input_shape= input_shape),\n",
    "    \n",
    "    Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal',kernel_regularizer=l1(0.01)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal',kernel_regularizer=l1(0.01)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal',kernel_regularizer=l1(0.01)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    Flatten(),\n",
    "    \n",
    "    Dense(128, activation='relu', kernel_initializer='he_normal',kernel_regularizer=l1(0.01)),\n",
    "    \n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "baseline_modeler(model, metrics = metrics, optimizer = 'SGD', num_epochs=50, early_stopping = None )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2ed530-2f2e-4163-8186-f2626ad5820c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# l2 regularizer, SGD optimizer, he_normal initializer\n",
    "\n",
    "model = Sequential([\n",
    "    \n",
    "    Rescaling(1./255, input_shape= input_shape),\n",
    "    \n",
    "    Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal',kernel_regularizer=l2(0.01)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal',kernel_regularizer=l2(0.01)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal',kernel_regularizer=l2(0.01)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    Flatten(),\n",
    "    \n",
    "    Dense(128, activation='relu', kernel_initializer='he_normal',kernel_regularizer=l2(0.01)),\n",
    "    \n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "baseline_modeler(model, metrics = metrics, optimizer = 'SGD', num_epochs=50, early_stopping = None )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e488e7e9-10f5-4e9e-82b8-78fb95730853",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Rescaling(1./255, input_shape=input_shape),\n",
    "    \n",
    "    Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal'),\n",
    "    Dropout(0.2),  # Example dropout after the first conv layer\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal'),\n",
    "    Dropout(0.3),  # Slightly higher dropout after the second conv layer\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal'),\n",
    "    Dropout(0.4),  # Even higher dropout after the third conv layer\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    Flatten(),\n",
    "    \n",
    "    Dense(128, activation='relu', kernel_initializer='he_normal'),\n",
    "    \n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "baseline_modeler(model, metrics = metrics, optimizer = 'SGD', num_epochs=300, early_stopping = early_stopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543d6ab1-326b-4ca5-89af-df600a06398f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropout after dense\n",
    "# SGD, he_normal\n",
    "early_stopping = EarlyStopping(monitor='val_loss', \n",
    "                               min_delta=0.001, \n",
    "                               patience=10, \n",
    "                               mode='min', \n",
    "                               restore_best_weights=True,\n",
    "                               verbose=1)\n",
    "model = Sequential([\n",
    "    Rescaling(1./255, input_shape=input_shape),\n",
    "    \n",
    "    Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    Flatten(),\n",
    "    \n",
    "    Dense(128, activation='relu', kernel_initializer='he_normal'),\n",
    "    Dropout(0.5),  \n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "baseline_modeler(model, metrics = metrics, optimizer = 'SGD', num_epochs=300, early_stopping = early_stopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cefe6cb-2a02-459b-813d-07047a14a5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropout after pooling\n",
    "# SGD, he_normal\n",
    "model = Sequential([\n",
    "    Rescaling(1./255, input_shape=input_shape),\n",
    "    \n",
    "    Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Dropout(0.25),  # Adding dropout after pooling\n",
    "    \n",
    "    Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Dropout(0.25),  # Repeating pattern for consistency\n",
    "    \n",
    "    Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Dropout(0.25),  # Maintaining dropout after pooling\n",
    "    \n",
    "    Flatten(),\n",
    "    \n",
    "    Dense(128, activation='relu', kernel_initializer='he_normal'),\n",
    "    \n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "baseline_modeler(model, metrics = metrics, optimizer = 'SGD', num_epochs=300, early_stopping = early_stopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e561c96-40bb-4d0b-ab59-34656e777db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Rescaling(1./255, input_shape=input_shape),\n",
    "    \n",
    "    Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    Flatten(),\n",
    "    \n",
    "    Dense(128, activation='relu', kernel_initializer='he_normal'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "baseline_modeler(model, metrics = metrics, optimizer = 'SGD', num_epochs=300, early_stopping = early_stopping )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f3a0de-637b-47ba-b83a-2d087833a74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increasing Filter Size in Conv2D Layers\n",
    "model = Sequential([\n",
    "    Rescaling(1./255, input_shape=input_shape),\n",
    "    \n",
    "    Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal'),\n",
    "    Dropout(0.1),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Dropout(0.1),\n",
    "    Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Dropout(0.1),\n",
    "    Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Dropout(0.1),\n",
    "    Flatten(),\n",
    "    \n",
    "    Dense(128, activation='relu', kernel_initializer='he_normal'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "baseline_modeler(model, metrics = metrics, optimizer = 'SGD', num_epochs=100, early_stopping = early_stopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d089e2b-9a7e-47da-b717-ba0726def75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Varying Kernel Size\n",
    "model = Sequential([\n",
    "    Rescaling(1./255, input_shape=input_shape),\n",
    "    \n",
    "    Conv2D(32, (5, 5), activation='relu', kernel_initializer='he_normal'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    Conv2D(64, (5, 5), activation='relu', kernel_initializer='he_normal'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    Conv2D(128, (5, 5), activation='relu', kernel_initializer='he_normal'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    Flatten(),\n",
    "    \n",
    "    Dense(128, activation='relu', kernel_initializer='he_normal'),\n",
    "    Dropout(0.5),  # Adding dropout before the final dense layer\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "baseline_modeler(model, metrics = metrics, optimizer = 'SGD', num_epochs=200, early_stopping = early_stopping )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f279ea-52ac-4e3c-a755-13cbdf0e7c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusting Units in Dense Layers\n",
    "model = Sequential([\n",
    "    Rescaling(1./255, input_shape=input_shape),\n",
    "    \n",
    "    Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    Flatten(),\n",
    "\n",
    "    Dense(256, activation='relu', kernel_initializer='he_normal'),#\n",
    "    Dropout(0.5), \n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "baseline_modeler(model, metrics = metrics, optimizer = 'SGD', num_epochs=200, early_stopping = early_stopping )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386fbeed-36c2-4bda-ad36-650ca17a9282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimenting with Dropout Rates\n",
    "model = Sequential([\n",
    "    Rescaling(1./255, input_shape=input_shape),\n",
    "    \n",
    "    Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    Flatten(),\n",
    "\n",
    "    Dense(128, activation='relu', kernel_initializer='he_normal'),#\n",
    "    Dropout(0.3), \n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "baseline_modeler(model, metrics = metrics, optimizer = 'SGD', num_epochs=200, early_stopping = early_stopping )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac11bd41-b8cc-44f0-b8e5-e653f1a97830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding an additional Conv2D + MaxPooling2D block\n",
    "model = Sequential([\n",
    "    Rescaling(1./255, input_shape=input_shape),\n",
    "    \n",
    "    Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "\n",
    "    Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    Flatten(),\n",
    "    \n",
    "    Dense(128, activation='relu', kernel_initializer='he_normal'),\n",
    "    Dropout(0.5),  # Adding dropout before the final dense layer\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "baseline_modeler(model, metrics = metrics, optimizer = 'SGD', num_epochs=200, early_stopping = early_stopping )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80e7b21-929e-4681-8d7f-0ed400f78e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding an additional Conv2D + MaxPooling2D block\n",
    "model = Sequential([\n",
    "    Rescaling(1./255, input_shape=input_shape),\n",
    "    \n",
    "    Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "\n",
    "    Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Dropout(0.4),  # Adding dropout after new Conv2D block\n",
    "    \n",
    "    Flatten(),\n",
    "    \n",
    "    Dense(128, activation='relu', kernel_initializer='he_normal'),\n",
    "    Dropout(0.5),  # Adding dropout before the final dense layer\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "baseline_modeler(model, metrics = metrics, optimizer = 'SGD', num_epochs=200, early_stopping = early_stopping )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75437856-91dd-4cbc-b24f-f53d666228a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding an additional Dense layer\n",
    "model = Sequential([\n",
    "    RandomZoom(1./255, input_shape=input_shape),\n",
    "    \n",
    "    Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    Flatten(),\n",
    "    \n",
    "    Dense(128, activation='relu', kernel_initializer='he_normal'),\n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation='relu', kernel_initializer='he_normal'),\n",
    "    Dropout(0.5),\n",
    "    \n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "baseline_modeler(model, metrics = metrics, optimizer = 'adam', num_epochs=200, early_stopping = early_stopping )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437ddc81-7486-45f8-999f-34b976b70fe8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f1d0d9-5789-4278-9d7e-af4fd00ea99f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd04084-a7bc-43c4-ac56-c47abfe079a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81df431f-6b07-4d3c-bb51-bf0a428420aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e74f201e-0f69-4b8e-a1d7-8daa0da269a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.engine.sequential.Sequential at 0x16d380fd0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Path to the saved model\n",
    "model_path = 'models/26_best_model.h5'\n",
    "\n",
    "# Load the model\n",
    "model = load_model(model_path)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caac1912-edd6-4b20-bd3d-92c63c0bda19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After loading, you can use the model to make predictions, evaluate it, etc.\n",
    "train_scores = model.evaluate(train_ds_scaled)\n",
    "val_scores = model.evaluate(val_ds_scaled)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
